<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><title>Comparing Apache Pulsar Streaming and Kafka Streaming - Pulsar Neighborhood</title><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="This article examines how Pulsar and Kafka approach streaming by discussing their architectures and services. After briefly reviewing the definition of streaming, we’ll start our discussion with Kafka, since it’s the tool against which all others in the category are compared. We’ll then outline how Pulsar differs from and solves the problems Kafka leaves unaddressed — specifically, how it handles streaming."><meta property="og:title" content="Comparing Apache Pulsar Streaming and Kafka Streaming - Pulsar Neighborhood"><meta property="og:type" content="article"><meta property="og:url" content="https://www.pulsar-neighborhood.io/articles/comparing-apache-pulsar-streaming-and-kafka-streaming/"><meta property="og:image" content="https://user-images.githubusercontent.com/16946028/176911260-f6072ced-ce7c-4876-b1db-1faf77fce1df.png"><meta property="og:description" content="This article examines how Pulsar and Kafka approach streaming by discussing their architectures and services. After briefly reviewing the definition of streaming, we’ll start our discussion with Kafka, since it’s the tool against which all others in the category are compared. We’ll then outline how Pulsar differs from and solves the problems Kafka leaves unaddressed — specifically, how it handles streaming."><meta name=twitter:site content="@pulsar_neighbor"><meta name=twitter:creator content="@pulsar_neighbor"><link rel=alternate type=application/rss+xml href=/index.xml title="Site Title"><meta name=google-site-verification content="m-1Z-0k0EvG9PD7R24pKuETSaBpEvqoXgOYT-0MtuPU"><link rel=icon href=https://www.pulsar-neighborhood.io/favicon.png><link rel=canonical href=https://www.pulsar-neighborhood.io/articles/comparing-apache-pulsar-streaming-and-kafka-streaming/><link rel=stylesheet href=/css/style.32ad96ee983b063b549f59db931afb8f0db25424bf2d687e1122748a7d183f73.css><style>.ctct-form-defaults{padding:15px!important}</style><script type=application/ld+json>[{"@context":"http://schema.org","@type":"WebPage","name":"Pulsar Neighborhood - Comparing Apache Pulsar Streaming and Kafka Streaming","description":"This article examines how Pulsar and Kafka approach streaming by discussing their architectures and services. After briefly reviewing the definition of streaming, we’ll start our discussion with Kafka, since it’s the tool against which all others in the category are compared. We’ll then outline how Pulsar differs from and solves the problems Kafka leaves unaddressed — specifically, how it handles streaming.","publisher":{"@type":"Organization","name":"Pulsar Neighborhood"}},{"@context":"http://schema.org","@type":"BlogPosting","image":"https://user-images.githubusercontent.com/16946028/176911260-f6072ced-ce7c-4876-b1db-1faf77fce1df.png","url":"https:\/\/www.pulsar-neighborhood.io\/articles\/comparing-apache-pulsar-streaming-and-kafka-streaming\/","publisher":"Pulsar Neighborhood","headline":"Comparing Apache Pulsar Streaming and Kafka Streaming","dateCreated":"2022-07-01 10:49:25 -0400 -0400","datePublished":"2022-07-01 10:49:25 -0400 -0400","dateModified":"2022-07-01 10:49:25 -0400 -0400","inLanguage":"en-US","isFamilyFriendly":"true","copyrightYear":"2022","copyrightHolder":"","author":{"@type":"Person","name":"Pulsar Neighborhood","url":"https://www.pulsar-neighborhood.io"},"mainEntityOfPage":"True","articleSection":["moving-to-pulsar"],"articleBody":"\u003cp\u003eThe distributed architectures and microservices of the average application often require massive flows of data that needs to be processed in real time. This demand for data-driven and scalable applications created an industry-wide push to create more capable event streaming solutions. Among these solutions are Pulsar and Kafka, which different companies initially developed before being open-sourced and adopted by \u003ca href=\u0022https:\/\/www.apache.org\/\u0022 target=\u0022_blank\u0022\u003eApache\u003c\/a\u003e.\u003c\/p\u003e\n\u003cp\u003eThis article examines how Pulsar and Kafka approach streaming by discussing their architectures and services. After briefly reviewing the definition of streaming, we’ll start our discussion with Kafka, since it’s the tool against which all others in the category are compared. We’ll then outline how Pulsar differs from and solves the problems Kafka leaves unaddressed — specifically, how it handles streaming.\u003c\/p\u003e\n\u003ch2 id=\u0022messaging\u0022\u003eMessaging\u003c\/h2\u003e\n\u003cp\u003eBecause new messaging solutions have appeared so rapidly, some of the terminology is still in flux. For example, the terms “streaming” and “stream processing” are often used interchangeably. However, they’re slightly distinct concepts — though they both relate to queueing, messaging, publish\/subscribe messaging, and batch processing. So, let’s define these terms to set a foundation for this discussion:\u003c\/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eMessaging\u003c\/strong\u003e is the transport, processing, and storage of data structures representing events or instructions, referred to as messages or records, typically communicating or coordinating components in a service or application. Message senders are called producers, and recipients are called consumers.\u003c\/p\u003e\n\u003c\/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eStreaming\u003c\/strong\u003e, also called streaming data or event streaming, is data that is continuously generated as small messages, typically from many sources simultaneously.\u003c\/p\u003e\n\u003c\/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eQueueing\u003c\/strong\u003e, also called point-to-point messaging or one-to-one messaging, is a form of asynchronous communication in which messages sent by a producer are stored in an intermediary channel (a queue) until they can be processed once by a single consumer and then deleted.\u003c\/p\u003e\n\u003c\/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eBatch processing\u003c\/strong\u003e is the transport, processing, and storage of aggregate data, which must be downloaded before it can be manipulated. In the context of messaging, batch processing typically refers to the processing of small batches of data as systems like message queues generate them. Batch processing typically operates with latencies of minutes or hours.\u003c\/p\u003e\n\u003c\/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003ePublish\/subscribe messaging\u003c\/strong\u003e, also called pub\/sub, is a form of asynchronous communication in which producers publish messages to an intermediary channel (a topic) without the knowledge of the message’s consumers. Consumers subscribe to topics to consume messages.\u003c\/p\u003e\n\u003c\/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eStream processing\u003c\/strong\u003e is the transport, processing, and storage of messages at the level of individual records or micro-batches of streaming data. It allows complex or parallelized operations on multiple records, such as joins and aggregations. Stream processing operates with latencies in seconds or milliseconds.\u003c\/p\u003e\n\u003c\/li\u003e\n\u003c\/ul\u003e\n\u003cp\u003eStreaming is typically contrasted against queueing, but it’s more accurate to contrast streaming or stream processing against batch processing. As you’ll discover, some solutions — Pulsar included — use hybrid models, implementing limited queue-like functionality in a streaming pub\/sub model.\u003c\/p\u003e\n\u003cp\u003eYou’ll also sometimes see real-time prefixed to event streaming or stream processing. In its strictest usage, real-time narrows these definitions to exclude micro-batching and emphasize the processing of infinite streams of individual events with millisecond latencies. Both concepts iterate on traditional, batch-processing-based messaging to function as a “nervous system” component for apps.\u003c\/p\u003e\n\u003cp\u003eKafka was developed at LinkedIn to provide real-time, low-latency data ingestion. Pulsar was developed as a Yahoo solution to address shortcomings in existing event streaming and open source messaging systems.\u003c\/p\u003e\n\u003ch2 id=\u0022kafka\u0022\u003eKafka\u003c\/h2\u003e\n\u003cp\u003eKafka offers durable stream storage so that events can be processed either in real-time or retrospectively and, in theory, runs in almost any working environment, ranging from a single bare-metal server to distributed container workloads in the cloud.\u003c\/p\u003e\n\u003cp\u003eYour application must either use an officially maintained Java library or one of the many community-provided clients for other languages to interface with one of Kafka’s five core APIs to implement Kafka:\u003c\/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe Producer API\u003c\/li\u003e\n\u003cli\u003eThe Consumer API\u003c\/li\u003e\n\u003cli\u003eThe Streams API\u003c\/li\u003e\n\u003cli\u003eThe Connect API\u003c\/li\u003e\n\u003cli\u003eThe Admin API\u003c\/li\u003e\n\u003c\/ul\u003e\n\u003cp\u003eThe Connect API is designed to build connectors to external services, and the Admin API lets you manage and inspect Kafka objects. Still, the Streams API contributes the highest level tools for streaming.\u003c\/p\u003e\n\u003cp\u003eTo understand how Kafka handles streaming using the Streams API, you first need an understanding of the standard Kafka architecture and how its producer and consumer libraries handle event streams.\u003c\/p\u003e\n\u003ch3 id=\u0022kafka-architecture\u0022\u003eKafka Architecture\u003c\/h3\u003e\n\u003cp\u003eKafka is structured as a set of servers and clients that communicate over TCP using a binary protocol. It uses a pub\/sub messaging model and Apache ZooKeeper to handle metadata and administrative functions.\u003c\/p\u003e\n\u003cp\u003eServer clusters can be massively distributed, but clusters spread over multiple geographic areas require additional configuration or extensions.\u003c\/p\u003e\n\u003cp\u003eClusters consist of servers acting in two roles:\u003c\/p\u003e\n\u003cul\u003e\n\u003cli\u003eSome servers maintain external data streams to connect the cluster to other Kafka clusters and your applications.\u003c\/li\u003e\n\u003cli\u003eThe other servers function as brokers, which form the storage layer and are ultimately the main nodes with which clients interact.\u003c\/li\u003e\n\u003c\/ul\u003e\n\u003cp\u003eClients interact with the server cluster to process event streams by writing to streams as producers or reading from streams as consumers.\u003c\/p\u003e\n\u003cp\u003eKafka defines its architecture using a few key components — some of which share names but not definitions — with concepts in Pulsar:\u003c\/p\u003e\n\u003cul\u003e\n\u003cli\u003eA message is an immutable carrier of state change or an identification. This may be a payment, a notification, a metric, or any other action or event that can be described.\u003c\/li\u003e\n\u003cli\u003eKafka stores each stream of messages on a broker as a sequence of records called a topic. Each topic stores events for a specified time rather than erasing them as soon as they’re consumed.\u003c\/li\u003e\n\u003cli\u003eEach topic is divided into sections called partitions.\u003c\/li\u003e\n\u003cli\u003eEach partition can be replicated, and the replicas are distributed to other cluster brokers.\u003c\/li\u003e\n\u003cli\u003eConsumers are typically organized in a consumer group.\u003c\/li\u003e\n\u003c\/ul\u003e\n\u003cp\u003eThere are several possible implementations of failover configurations using replicas, but the most straightforward uses a leader\/replica arrangement. Clients reading or writing to a partition only interact with the leader partition, which then updates all its replicas.\u003c\/p\u003e\n\u003ch3 id=\u0022implications-for-streaming\u0022\u003eImplications for Streaming\u003c\/h3\u003e\n\u003cp\u003eKafka’s design choices have a few implications for streaming.\u003c\/p\u003e\n\u003cp\u003eReplicating partitions gives Kafka high throughput. Consumers can read from multiple partitions in parallel, which means that a group’s parallelism is determined by the number of partitions in a topic. You can set up systems to provide massive parallelism on the server-side for clients to take advantage of, and it’s easy to add large numbers of consumers without impacting performance.\u003c\/p\u003e\n\u003cp\u003eHowever, each broker must be able to store an entire replica, so each node in a cluster requires a large storage volume. Additionally, Kafka can perform well at large scales, but the scaling process requires adding partitions and rebalancing them among brokers — typically a very error-prone and I\/O-costly procedure.\u003c\/p\u003e\n\u003cp\u003eReplicas also ensure that even if one or more servers fail, the cluster continues to run without any data loss. Lost replicas may affect total cluster throughput, but they can be replaced. If a server containing a topic’s leader partition fails, a new leader is selected from that partition’s replica pool on other servers.\u003c\/p\u003e\n\u003cp\u003eKafka topics decouple producers from consumers in a comparable way to most messaging systems, allowing clients to consume data at their own pace.\u003c\/p\u003e\n\u003cp\u003eKafka’s pull-based messaging for clients and its durable storage of messages on brokers ensure message replayability. For example, you can guarantee a consumer downstream receives each message at least once.\u003c\/p\u003e\n\u003cp\u003eSo, Kafka provides a simple, fast, and durable foundation for streaming.\u003c\/p\u003e\n\u003ch3 id=\u0022kafka-stream-processing\u0022\u003eKafka Stream Processing\u003c\/h3\u003e\n\u003cp\u003eIn addition to consuming a stream and then performing a simple action on it, you may want to perform complex business logic on it and, optionally, publish it back into Kafka.\u003c\/p\u003e\n\u003cp\u003eIn theory, you could develop a stream processing application around a chain of clients using Producer and Consumer APIs. But the Streams API provides a simpler way to implement functionality like transforms directly, joins, and aggregations in a client and then send it downstream or back to the cluster.\u003c\/p\u003e\n\u003cp\u003eKafka Streams implements its parallelism by sectioning input streams and treating them as separate logical units for processing:\u003c\/p\u003e\n\u003cp\u003e\u003cimg src=\u0022https:\/\/user-images.githubusercontent.com\/16946028\/176911506-41bcfef4-52fe-4871-9e3e-08baf23464d5.png\u0022 alt=\u0022image1\u0022\u003e\u003c\/p\u003e\n\u003cul\u003e\n\u003cli\u003eEach Kafka message maps to a data record.\u003c\/li\u003e\n\u003cli\u003eEach Kafka topic partition maps to a stream partition.\u003c\/li\u003e\n\u003cli\u003eKafka Streams creates and assigns a task for each stream partition. The correlation between a specific task and its partition never changes.\u003c\/li\u003e\n\u003cli\u003eEach task configures its own buffer and processor topology based on its assigned partition and processes data records one at a time.\u003c\/li\u003e\n\u003cli\u003eYou can assign one or more tasks to a thread to group, separate, and parallelize task processing. Threads are independent and require no coordination.\u003c\/li\u003e\n\u003c\/ul\u003e\n\u003cp\u003eThe independent nature of threads and their capability to guarantee \u003ca href=\u0022https:\/\/kafka.apache.org\/documentation\/#semantics\u0022 target=\u0022_blank\u0022\u003eexactly-once processing\u003c\/a\u003e make Kafka suitable for sensitive applications like financial transaction processing.\u003c\/p\u003e\n\u003ch2 id=\u0022pulsar\u0022\u003ePulsar\u003c\/h2\u003e\n\u003cp\u003ePulsar combines the features of a pub\/sub streaming model with those of a traditional distributed messaging system. It doesn’t distinguish between pub\/sub messaging and streaming, and its stream processing functionality is integrated and deployed directly on nodes. Pulsar also natively supports multi-tenancy, which it implements above the namespace level, and geo-replication, which it implements at the instance level.\u003c\/p\u003e\n\u003cp\u003eA defining feature of Pulsar is its additional separation of concerns beyond what Kafka achieves by decoupling message routing and service from message storage.\u003c\/p\u003e\n\u003cp\u003eAdditionally, Pulsar pushes messages to consumers rather than waiting for them to pull data. When they have finished processing a message, consumers in Pulsar send an acknowledgement, and the message is deleted.\u003c\/p\u003e\n\u003cp\u003eLet’s examine how Pulsar’s architecture differs from Kafka’s to understand how it handles streaming.\u003c\/p\u003e\n\u003ch3 id=\u0022pulsar-architecture\u0022\u003ePulsar Architecture\u003c\/h3\u003e\n\u003cp\u003ePulsar is structured as a layered architecture containing servers and clients that communicate over TCP using a binary protocol.\u003c\/p\u003e\n\u003cp\u003eA pulsar instance contains one or more clusters and uses a ZooKeeper cluster called the configuration store to provide native geo-replication between clusters.\u003c\/p\u003e\n\u003cp\u003eA Pulsar cluster uses servers at its edge called brokers to interface with clients. Pulsar Brokers perform two primary functions:\u003c\/p\u003e\n\u003cul\u003e\n\u003cli\u003eThey connect clients to topics through an HTTP server exposing a REST interface.\u003c\/li\u003e\n\u003cli\u003eBrokers transfer data through an asynchronous TCP server called a dispatcher.\u003c\/li\u003e\n\u003c\/ul\u003e\n\u003cp\u003ePulsar relies on Apache BookKeeper for distributed persistent storage, so it uses a few BookKeeper concepts:\u003c\/p\u003e\n\u003cul\u003e\n\u003cli\u003eAn event or message maps to a log unit called an entry.\u003c\/li\u003e\n\u003cli\u003eA stream of entries is appended to the end of a topic’s ledger. Ledgers are append-only — entries can’t be modified once they’re written to a ledger — and are periodically closed to start a new ledger.\u003c\/li\u003e\n\u003cli\u003eLedgers are split into fragments and distributed among servers called bookies.\u003c\/li\u003e\n\u003cli\u003eAn entire ledger is distributed across an ensemble of bookies.\u003c\/li\u003e\n\u003cli\u003ePulsar uses a cluster-specific ZooKeeper cluster to manage bookies.\u003c\/li\u003e\n\u003c\/ul\u003e\n\u003cp\u003ePulsar brokers can operate statelessly — they act as intermediaries between endpoints but don’t store persistent data — although, in practice, Pulsar uses a sort of limited statefulness for better performance:\u003c\/p\u003e\n\u003cul\u003e\n\u003cli\u003eEach topic is owned by one broker that handles all read and write functions for that topic.\u003c\/li\u003e\n\u003cli\u003eA broker persists data in a topic by passing it to an ensemble. In the simplest case, a topic’s most recent entries exist in a single fragment, but fragments — and sometimes entries — are frequently distributed across multiple bookies.\u003c\/li\u003e\n\u003cli\u003eA broker can cache a topic’s managed ledger, which abstracts a single topic’s storage layer. A managed ledger contains a single writer appending to the ledger and multiple cursors representing consumers with their own positions in the stream.\u003c\/li\u003e\n\u003c\/ul\u003e\n\u003ch3 id=\u0022persistent-messaging\u0022\u003ePersistent Messaging\u003c\/h3\u003e\n\u003cp\u003ePulsar guarantees that messages that reach a Pulsar broker will be delivered to their intended targets.\u003c\/p\u003e\n\u003cp\u003eIn contrast to Kafka’s handling of undelivered messages, all messages in a Pulsar topic are retained by default, even if a consumer is disconnected. Pulsar only discards messages when a consumer acknowledges successful processing, which it does by default.\u003c\/p\u003e\n\u003cp\u003eHowever, you can configure this behavior by using message retention to store instead messages that have been acknowledged or message expiry to set a duration past which unacknowledged messages will be deleted anyway.\u003c\/p\u003e\n\u003cp\u003ePulsar determines how messages are delivered by using one of three subscription modes for each subscription:\u003c\/p\u003e\n\u003cul\u003e\n\u003cli\u003eExclusive mode permits only a single consumer to attach to a subscription.\u003c\/li\u003e\n\u003cli\u003eShared or round-robin mode distributes messages between multiple consumers.\u003c\/li\u003e\n\u003cli\u003eFailover mode permits multiple consumers but only delivers messages to one consumer, called the master consumer. When the master consumer disconnects, the next consumer in line becomes the master consumer.\u003c\/li\u003e\n\u003c\/ul\u003e\n\u003cp\u003eAdditionally, Pulsar can increase throughput for highly active topics by applying its own form of partitioning, which it implements internally as sub-topics, each with its own broker. Messages published to a partitioned topic are automatically routed to the right broker by Pulsar.\u003c\/p\u003e\n\u003ch3 id=\u0022implications-for-streaming-1\u0022\u003eImplications for Streaming\u003c\/h3\u003e\n\u003cp\u003eBy decoupling the storage layer from message routing and using tiered storage, Pulsar achieves very low end-to-end latency while still guaranteeing message delivery. Cached managed ledgers allow clients to interact with Pulsar clusters without incurring disk reads or writes — sometimes achieving latencies under five milliseconds. Clients needing to read older messages in a backlog can still easily access ledgers offloaded to less expensive storage outside the ensemble.\u003c\/p\u003e\n\u003cp\u003eOverall, storage in Pulsar is more easily scalable but not always as fast as Kafka’s sequential disk reads. Because data is often striped across multiple bookies, it isn’t easy to achieve comparable read speeds for large datasets.\u003c\/p\u003e\n\u003cp\u003eHowever, topics are limited in size only by the total storage of an entire cluster of bookies, rather than being limited by the broker with the least storage in a Kafka cluster. Additionally, because Pulsar doesn’t rely on replicas, it can easily add more bookies to scale out to over a million topics per instance without needing to copy data or rebalance brokers.\u003c\/p\u003e\n\u003cp\u003eCombined with Pulsar’s built-in geo-replication and multi-tenancy, these characteristics make it a strong choice for massively distributed applications that operate on extremely large files or large numbers of small messages, require guaranteed delivery, and may need characteristics of a queuing platform. However, its inability to provide exactly-once processing makes it unsuitable for highly sensitive applications, like those that handle financial transactions.\u003c\/p\u003e\n\u003ch3 id=\u0022pulsar-stream-processing\u0022\u003ePulsar Stream Processing\u003c\/h3\u003e\n\u003cp\u003ePulsar provides integrated stream processing capability through a lightweight computing engine called Pulsar Functions. Functions is a serverless framework that can apply logic comparable to lambda-style functions directly on the broker without the need for an external system. Functions can process an input and then publish the output to a topic in Pulsar, write it directly to BookKeeper, or write to a log topic.\u003c\/p\u003e\n\u003cp\u003eFunctions is a much lighter implementation of stream processing than Kafka Streams, and its state management and DAG flow capabilities are not as powerful. However, Functions provides its functionality built into every broker by default. If you need additional, specific computing capabilities, Pulsar’s support for other protocols, such as RabbitMQ, AMQP, Kafka, and Presto, lets it integrate with other tools to supplement its functionality.\u003c\/p\u003e\n\u003ch2 id=\u0022conclusion\u0022\u003eConclusion\u003c\/h2\u003e\n\u003cp\u003eApache Kafka and Apache Pulsar both offer event streaming and stream processing capabilities, but they approach the task in different ways.\u003c\/p\u003e\n\u003cp\u003eKafka’s replica-based topic storage, massive parallelization potential, and powerful stream processing capabilities make it a strong choice for systems that require high resilience and reliable storage but rely on simpler messaging patterns.\u003c\/p\u003e\n\u003cp\u003eDespite excellent performance at large scales, Kafka suffers from scaling inertia: scaling up is a delicate, resource-intensive process requiring the balanced addition of large storage volumes across an entire cluster.\u003c\/p\u003e\n\u003cp\u003eKafka is well-suited for the financial sector, IoT applications, operational metrics, and autonomous vehicles.\u003c\/p\u003e\n\u003cp\u003ePulsar offers a more flexible solution with integrated geo-replication, multi-tenancy, decoupled storage and message routing layers, and integrated computing capability. It offers modes of messaging functionality that satisfy the pattern requirements of ultra-low-latency real-time event processing and batch processing, with no data loss across intrinsically and easily highly scalable systems.\u003c\/p\u003e\n\u003cp\u003eAlthough it can handle arbitrarily large files and offers tiered storage, Pulsar is generally \u003ca href=\u0022https:\/\/www.confluent.io\/blog\/kafka-fastest-messaging-system\/\u0022 target=\u0022_blank\u0022\u003eslightly less performant\u003c\/a\u003e than \u003ca href=\u0022https:\/\/www.kai-waehner.de\/blog\/2020\/06\/09\/apache-kafka-versus-apache-pulsar-event-streaming-comparison-features-myths-explored\/#myth-6-pulsars-performance-is-much-better-than-kafkas\u0022 target=\u0022_blank\u0022\u003eKafka\u003c\/a\u003e when accessing messages outside of a broker’s managed ledger. It also can’t guarantee exactly-once processing, and its native solution for stream processing is limited to simple functionality.\u003c\/p\u003e\n\u003cp\u003ePulsar is a strong choice for microservices, autonomous vehicles, instant messaging, and analytics.\u003c\/p\u003e\n"}]</script></head><body class='page bg-secondary'><div id=main-menu-mobile class=main-menu-mobile><ul></ul></div><div class=wrapper><div class='header sticky-top bg-secondary'><div class=container><div class=logo><a href=https://www.pulsar-neighborhood.io><img src=/svg/Pulsar-Neighborhood-Logo.svg class=img-fluid></a></div><div class=logo-mobile><a href=https://www.pulsar-neighborhood.io><img src=/svg/Neighborhood-Icon.svg class=img-fluid></a></div><div id=main-menu class=main-menu><ul><li><a href=/about><span>About The Pulsar Neighborhood</span></a></li><li><a href=https://pulsar.apache.org/ target=_blank><span>About Apache Pulsar</span></a></li><li><a href=https://pulsar.apache.org/docs/en/standalone/ target=_blank><span></span>Project Documentation</span></a></li><li><a href=/index.xml><img src=/svg/square-rss-solid.svg title="Subscribe to The Pulsar Neighborhood feed" class=img-fluid style=height:30px></a></li></ul></div><button id=toggle-main-menu-mobile class="hamburger hamburger--slider" type=button>
<span class=hamburger-box><span class=hamburger-inner></span></span></button></div></div><div class="container pt-2 pb-3"><div class=row><div class="col-0 col-md-2"><div class=sidebar><div class=mb-2><a href=/guides/getting-started><div class="col-12 btn btn-success">Get Started with Pulsar</div></a></div><div class="spotlight mb-2 border border-primary rounded"><div class="row bg-primary no-gutters"><div class="col-12 text-secondary p-1">&nbsp;Find Topics</div></div><div class=p-1><div class="row no-gutters"><div class="col-12 mb-1"><h4>Level</h4><ul><li><input class=form-check-input type=checkbox id=100 onclick=toggleFilter(filterOptions.levelFilters,this.id)><label class=form-check-label for=100>&nbsp;Beginner (100)</label></li><li><input class=form-check-input type=checkbox id=200 onclick=toggleFilter(filterOptions.levelFilters,this.id)><label class=form-check-label for=200>&nbsp;Intermediate (200)</label></li><li><input class=form-check-input type=checkbox id=300 onclick=toggleFilter(filterOptions.levelFilters,this.id)><label class=form-check-label for=300>&nbsp;Advanced (300)</label></li></ul></div><div class="col-12 mb-1"><h4>Category</h4><ul><li><input class=form-check-input type=checkbox id=at-the-edge onclick=toggleFilter(filterOptions.categoryFilters,this.id)>
<label class=form-check-label for=at-the-edge>&nbsp;At the Edge [3]</label></li><li><input class=form-check-input type=checkbox id=cluster-administration onclick=toggleFilter(filterOptions.categoryFilters,this.id)>
<label class=form-check-label for=cluster-administration>&nbsp;Cluster Administration [2]</label></li><li><input class=form-check-input type=checkbox id=getting-started onclick=toggleFilter(filterOptions.categoryFilters,this.id)>
<label class=form-check-label for=getting-started>&nbsp;Getting Started [14]</label></li><li><input class=form-check-input type=checkbox id=machine-learning onclick=toggleFilter(filterOptions.categoryFilters,this.id)>
<label class=form-check-label for=machine-learning>&nbsp;Machine Learning [2]</label></li><li><input class=form-check-input type=checkbox id=moving-to-pulsar onclick=toggleFilter(filterOptions.categoryFilters,this.id)>
<label class=form-check-label for=moving-to-pulsar>&nbsp;Moving to Pulsar [9]</label></li><li><input class=form-check-input type=checkbox id=newsletter onclick=toggleFilter(filterOptions.categoryFilters,this.id)>
<label class=form-check-label for=newsletter>&nbsp;Newsletter [7]</label></li><li><input class=form-check-input type=checkbox id=project-news onclick=toggleFilter(filterOptions.categoryFilters,this.id)>
<label class=form-check-label for=project-news>&nbsp;Project News [3]</label></li><li><input class=form-check-input type=checkbox id=pulsar-architecture onclick=toggleFilter(filterOptions.categoryFilters,this.id)>
<label class=form-check-label for=pulsar-architecture>&nbsp;Pulsar Architecture [10]</label></li><li><input class=form-check-input type=checkbox id=pulsar-components onclick=toggleFilter(filterOptions.categoryFilters,this.id)>
<label class=form-check-label for=pulsar-components>&nbsp;Pulsar Components [7]</label></li><li><input class=form-check-input type=checkbox id=use-cases onclick=toggleFilter(filterOptions.categoryFilters,this.id)>
<label class=form-check-label for=use-cases>&nbsp;Use Cases [4]</label></li></ul></div><div class=col-12><h4>View All</h4><div class="mt-1 pb-1"><a href=/articles>Articles</a>
&nbsp;|&nbsp;<a href=/guides>Guides</a>
&nbsp;|&nbsp;<a href=/videos>Videos</a></div></div></div></div></div></div></div><div class="col-12 col-md-8"><div class=container><header class=mb-3><div class=pb-2><img src=https://user-images.githubusercontent.com/16946028/176911260-f6072ced-ce7c-4876-b1db-1faf77fce1df.png alt class=img-fluid style=max-width:85%;max-height:200px></div><h1>Comparing Apache Pulsar Streaming and Kafka Streaming</h1></header><div class="row mb-3 no-gutters border-top border-bottom"><div class=col-2><ul class="list-group list-group-horizontal mt-1 mb-1"><li class="list-group-item flex-fill border-0 p-0 mr-3"><a href="https://www.linkedin.com/sharing/share-offsite/?mini=true&url=https%3a%2f%2fwww.pulsar-neighborhood.io%2farticles%2fcomparing-apache-pulsar-streaming-and-kafka-streaming%2f&title=Comparing%20Apache%20Pulsar%20Streaming%20and%20Kafka%20Streaming" target=_blank><img src=/svg/linkedin-in-brands.svg title="Share this on LinkedIn" class=img-fluid></a></li><li class="list-group-item flex-fill border-0 p-0 mr-3"><a target=_blank href="https://twitter.com/intent/tweet?text=Comparing%20Apache%20Pulsar%20Streaming%20and%20Kafka%20Streaming&url=https%3a%2f%2fwww.pulsar-neighborhood.io%2farticles%2fcomparing-apache-pulsar-streaming-and-kafka-streaming%2f&hashtags=apachepulsar&via=pulsar_neighbor"><img src=/svg/twitter-brands.svg title="Share this on Twitter" class=img-fluid></a></li></ul></div><div class="col text-right"><span class=text-muted>Author:</span> Pulsar Neighborhood<br><span class=text-muted>Published:</span> July 1, 2022</div></div><article><p>The distributed architectures and microservices of the average application often require massive flows of data that needs to be processed in real time. This demand for data-driven and scalable applications created an industry-wide push to create more capable event streaming solutions. Among these solutions are Pulsar and Kafka, which different companies initially developed before being open-sourced and adopted by <a href=https://www.apache.org/ target=_blank>Apache</a>.</p><p>This article examines how Pulsar and Kafka approach streaming by discussing their architectures and services. After briefly reviewing the definition of streaming, we’ll start our discussion with Kafka, since it’s the tool against which all others in the category are compared. We’ll then outline how Pulsar differs from and solves the problems Kafka leaves unaddressed — specifically, how it handles streaming.</p><h2 id=messaging>Messaging</h2><p>Because new messaging solutions have appeared so rapidly, some of the terminology is still in flux. For example, the terms “streaming” and “stream processing” are often used interchangeably. However, they’re slightly distinct concepts — though they both relate to queueing, messaging, publish/subscribe messaging, and batch processing. So, let’s define these terms to set a foundation for this discussion:</p><ul><li><p><strong>Messaging</strong> is the transport, processing, and storage of data structures representing events or instructions, referred to as messages or records, typically communicating or coordinating components in a service or application. Message senders are called producers, and recipients are called consumers.</p></li><li><p><strong>Streaming</strong>, also called streaming data or event streaming, is data that is continuously generated as small messages, typically from many sources simultaneously.</p></li><li><p><strong>Queueing</strong>, also called point-to-point messaging or one-to-one messaging, is a form of asynchronous communication in which messages sent by a producer are stored in an intermediary channel (a queue) until they can be processed once by a single consumer and then deleted.</p></li><li><p><strong>Batch processing</strong> is the transport, processing, and storage of aggregate data, which must be downloaded before it can be manipulated. In the context of messaging, batch processing typically refers to the processing of small batches of data as systems like message queues generate them. Batch processing typically operates with latencies of minutes or hours.</p></li><li><p><strong>Publish/subscribe messaging</strong>, also called pub/sub, is a form of asynchronous communication in which producers publish messages to an intermediary channel (a topic) without the knowledge of the message’s consumers. Consumers subscribe to topics to consume messages.</p></li><li><p><strong>Stream processing</strong> is the transport, processing, and storage of messages at the level of individual records or micro-batches of streaming data. It allows complex or parallelized operations on multiple records, such as joins and aggregations. Stream processing operates with latencies in seconds or milliseconds.</p></li></ul><p>Streaming is typically contrasted against queueing, but it’s more accurate to contrast streaming or stream processing against batch processing. As you’ll discover, some solutions — Pulsar included — use hybrid models, implementing limited queue-like functionality in a streaming pub/sub model.</p><p>You’ll also sometimes see real-time prefixed to event streaming or stream processing. In its strictest usage, real-time narrows these definitions to exclude micro-batching and emphasize the processing of infinite streams of individual events with millisecond latencies. Both concepts iterate on traditional, batch-processing-based messaging to function as a “nervous system” component for apps.</p><p>Kafka was developed at LinkedIn to provide real-time, low-latency data ingestion. Pulsar was developed as a Yahoo solution to address shortcomings in existing event streaming and open source messaging systems.</p><h2 id=kafka>Kafka</h2><p>Kafka offers durable stream storage so that events can be processed either in real-time or retrospectively and, in theory, runs in almost any working environment, ranging from a single bare-metal server to distributed container workloads in the cloud.</p><p>Your application must either use an officially maintained Java library or one of the many community-provided clients for other languages to interface with one of Kafka’s five core APIs to implement Kafka:</p><ul><li>The Producer API</li><li>The Consumer API</li><li>The Streams API</li><li>The Connect API</li><li>The Admin API</li></ul><p>The Connect API is designed to build connectors to external services, and the Admin API lets you manage and inspect Kafka objects. Still, the Streams API contributes the highest level tools for streaming.</p><p>To understand how Kafka handles streaming using the Streams API, you first need an understanding of the standard Kafka architecture and how its producer and consumer libraries handle event streams.</p><h3 id=kafka-architecture>Kafka Architecture</h3><p>Kafka is structured as a set of servers and clients that communicate over TCP using a binary protocol. It uses a pub/sub messaging model and Apache ZooKeeper to handle metadata and administrative functions.</p><p>Server clusters can be massively distributed, but clusters spread over multiple geographic areas require additional configuration or extensions.</p><p>Clusters consist of servers acting in two roles:</p><ul><li>Some servers maintain external data streams to connect the cluster to other Kafka clusters and your applications.</li><li>The other servers function as brokers, which form the storage layer and are ultimately the main nodes with which clients interact.</li></ul><p>Clients interact with the server cluster to process event streams by writing to streams as producers or reading from streams as consumers.</p><p>Kafka defines its architecture using a few key components — some of which share names but not definitions — with concepts in Pulsar:</p><ul><li>A message is an immutable carrier of state change or an identification. This may be a payment, a notification, a metric, or any other action or event that can be described.</li><li>Kafka stores each stream of messages on a broker as a sequence of records called a topic. Each topic stores events for a specified time rather than erasing them as soon as they’re consumed.</li><li>Each topic is divided into sections called partitions.</li><li>Each partition can be replicated, and the replicas are distributed to other cluster brokers.</li><li>Consumers are typically organized in a consumer group.</li></ul><p>There are several possible implementations of failover configurations using replicas, but the most straightforward uses a leader/replica arrangement. Clients reading or writing to a partition only interact with the leader partition, which then updates all its replicas.</p><h3 id=implications-for-streaming>Implications for Streaming</h3><p>Kafka’s design choices have a few implications for streaming.</p><p>Replicating partitions gives Kafka high throughput. Consumers can read from multiple partitions in parallel, which means that a group’s parallelism is determined by the number of partitions in a topic. You can set up systems to provide massive parallelism on the server-side for clients to take advantage of, and it’s easy to add large numbers of consumers without impacting performance.</p><p>However, each broker must be able to store an entire replica, so each node in a cluster requires a large storage volume. Additionally, Kafka can perform well at large scales, but the scaling process requires adding partitions and rebalancing them among brokers — typically a very error-prone and I/O-costly procedure.</p><p>Replicas also ensure that even if one or more servers fail, the cluster continues to run without any data loss. Lost replicas may affect total cluster throughput, but they can be replaced. If a server containing a topic’s leader partition fails, a new leader is selected from that partition’s replica pool on other servers.</p><p>Kafka topics decouple producers from consumers in a comparable way to most messaging systems, allowing clients to consume data at their own pace.</p><p>Kafka’s pull-based messaging for clients and its durable storage of messages on brokers ensure message replayability. For example, you can guarantee a consumer downstream receives each message at least once.</p><p>So, Kafka provides a simple, fast, and durable foundation for streaming.</p><h3 id=kafka-stream-processing>Kafka Stream Processing</h3><p>In addition to consuming a stream and then performing a simple action on it, you may want to perform complex business logic on it and, optionally, publish it back into Kafka.</p><p>In theory, you could develop a stream processing application around a chain of clients using Producer and Consumer APIs. But the Streams API provides a simpler way to implement functionality like transforms directly, joins, and aggregations in a client and then send it downstream or back to the cluster.</p><p>Kafka Streams implements its parallelism by sectioning input streams and treating them as separate logical units for processing:</p><p><img src=https://user-images.githubusercontent.com/16946028/176911506-41bcfef4-52fe-4871-9e3e-08baf23464d5.png alt=image1></p><ul><li>Each Kafka message maps to a data record.</li><li>Each Kafka topic partition maps to a stream partition.</li><li>Kafka Streams creates and assigns a task for each stream partition. The correlation between a specific task and its partition never changes.</li><li>Each task configures its own buffer and processor topology based on its assigned partition and processes data records one at a time.</li><li>You can assign one or more tasks to a thread to group, separate, and parallelize task processing. Threads are independent and require no coordination.</li></ul><p>The independent nature of threads and their capability to guarantee <a href=https://kafka.apache.org/documentation/#semantics target=_blank>exactly-once processing</a> make Kafka suitable for sensitive applications like financial transaction processing.</p><h2 id=pulsar>Pulsar</h2><p>Pulsar combines the features of a pub/sub streaming model with those of a traditional distributed messaging system. It doesn’t distinguish between pub/sub messaging and streaming, and its stream processing functionality is integrated and deployed directly on nodes. Pulsar also natively supports multi-tenancy, which it implements above the namespace level, and geo-replication, which it implements at the instance level.</p><p>A defining feature of Pulsar is its additional separation of concerns beyond what Kafka achieves by decoupling message routing and service from message storage.</p><p>Additionally, Pulsar pushes messages to consumers rather than waiting for them to pull data. When they have finished processing a message, consumers in Pulsar send an acknowledgement, and the message is deleted.</p><p>Let’s examine how Pulsar’s architecture differs from Kafka’s to understand how it handles streaming.</p><h3 id=pulsar-architecture>Pulsar Architecture</h3><p>Pulsar is structured as a layered architecture containing servers and clients that communicate over TCP using a binary protocol.</p><p>A pulsar instance contains one or more clusters and uses a ZooKeeper cluster called the configuration store to provide native geo-replication between clusters.</p><p>A Pulsar cluster uses servers at its edge called brokers to interface with clients. Pulsar Brokers perform two primary functions:</p><ul><li>They connect clients to topics through an HTTP server exposing a REST interface.</li><li>Brokers transfer data through an asynchronous TCP server called a dispatcher.</li></ul><p>Pulsar relies on Apache BookKeeper for distributed persistent storage, so it uses a few BookKeeper concepts:</p><ul><li>An event or message maps to a log unit called an entry.</li><li>A stream of entries is appended to the end of a topic’s ledger. Ledgers are append-only — entries can’t be modified once they’re written to a ledger — and are periodically closed to start a new ledger.</li><li>Ledgers are split into fragments and distributed among servers called bookies.</li><li>An entire ledger is distributed across an ensemble of bookies.</li><li>Pulsar uses a cluster-specific ZooKeeper cluster to manage bookies.</li></ul><p>Pulsar brokers can operate statelessly — they act as intermediaries between endpoints but don’t store persistent data — although, in practice, Pulsar uses a sort of limited statefulness for better performance:</p><ul><li>Each topic is owned by one broker that handles all read and write functions for that topic.</li><li>A broker persists data in a topic by passing it to an ensemble. In the simplest case, a topic’s most recent entries exist in a single fragment, but fragments — and sometimes entries — are frequently distributed across multiple bookies.</li><li>A broker can cache a topic’s managed ledger, which abstracts a single topic’s storage layer. A managed ledger contains a single writer appending to the ledger and multiple cursors representing consumers with their own positions in the stream.</li></ul><h3 id=persistent-messaging>Persistent Messaging</h3><p>Pulsar guarantees that messages that reach a Pulsar broker will be delivered to their intended targets.</p><p>In contrast to Kafka’s handling of undelivered messages, all messages in a Pulsar topic are retained by default, even if a consumer is disconnected. Pulsar only discards messages when a consumer acknowledges successful processing, which it does by default.</p><p>However, you can configure this behavior by using message retention to store instead messages that have been acknowledged or message expiry to set a duration past which unacknowledged messages will be deleted anyway.</p><p>Pulsar determines how messages are delivered by using one of three subscription modes for each subscription:</p><ul><li>Exclusive mode permits only a single consumer to attach to a subscription.</li><li>Shared or round-robin mode distributes messages between multiple consumers.</li><li>Failover mode permits multiple consumers but only delivers messages to one consumer, called the master consumer. When the master consumer disconnects, the next consumer in line becomes the master consumer.</li></ul><p>Additionally, Pulsar can increase throughput for highly active topics by applying its own form of partitioning, which it implements internally as sub-topics, each with its own broker. Messages published to a partitioned topic are automatically routed to the right broker by Pulsar.</p><h3 id=implications-for-streaming-1>Implications for Streaming</h3><p>By decoupling the storage layer from message routing and using tiered storage, Pulsar achieves very low end-to-end latency while still guaranteeing message delivery. Cached managed ledgers allow clients to interact with Pulsar clusters without incurring disk reads or writes — sometimes achieving latencies under five milliseconds. Clients needing to read older messages in a backlog can still easily access ledgers offloaded to less expensive storage outside the ensemble.</p><p>Overall, storage in Pulsar is more easily scalable but not always as fast as Kafka’s sequential disk reads. Because data is often striped across multiple bookies, it isn’t easy to achieve comparable read speeds for large datasets.</p><p>However, topics are limited in size only by the total storage of an entire cluster of bookies, rather than being limited by the broker with the least storage in a Kafka cluster. Additionally, because Pulsar doesn’t rely on replicas, it can easily add more bookies to scale out to over a million topics per instance without needing to copy data or rebalance brokers.</p><p>Combined with Pulsar’s built-in geo-replication and multi-tenancy, these characteristics make it a strong choice for massively distributed applications that operate on extremely large files or large numbers of small messages, require guaranteed delivery, and may need characteristics of a queuing platform. However, its inability to provide exactly-once processing makes it unsuitable for highly sensitive applications, like those that handle financial transactions.</p><h3 id=pulsar-stream-processing>Pulsar Stream Processing</h3><p>Pulsar provides integrated stream processing capability through a lightweight computing engine called Pulsar Functions. Functions is a serverless framework that can apply logic comparable to lambda-style functions directly on the broker without the need for an external system. Functions can process an input and then publish the output to a topic in Pulsar, write it directly to BookKeeper, or write to a log topic.</p><p>Functions is a much lighter implementation of stream processing than Kafka Streams, and its state management and DAG flow capabilities are not as powerful. However, Functions provides its functionality built into every broker by default. If you need additional, specific computing capabilities, Pulsar’s support for other protocols, such as RabbitMQ, AMQP, Kafka, and Presto, lets it integrate with other tools to supplement its functionality.</p><h2 id=conclusion>Conclusion</h2><p>Apache Kafka and Apache Pulsar both offer event streaming and stream processing capabilities, but they approach the task in different ways.</p><p>Kafka’s replica-based topic storage, massive parallelization potential, and powerful stream processing capabilities make it a strong choice for systems that require high resilience and reliable storage but rely on simpler messaging patterns.</p><p>Despite excellent performance at large scales, Kafka suffers from scaling inertia: scaling up is a delicate, resource-intensive process requiring the balanced addition of large storage volumes across an entire cluster.</p><p>Kafka is well-suited for the financial sector, IoT applications, operational metrics, and autonomous vehicles.</p><p>Pulsar offers a more flexible solution with integrated geo-replication, multi-tenancy, decoupled storage and message routing layers, and integrated computing capability. It offers modes of messaging functionality that satisfy the pattern requirements of ultra-low-latency real-time event processing and batch processing, with no data loss across intrinsically and easily highly scalable systems.</p><p>Although it can handle arbitrarily large files and offers tiered storage, Pulsar is generally <a href=https://www.confluent.io/blog/kafka-fastest-messaging-system/ target=_blank>slightly less performant</a> than <a href=https://www.kai-waehner.de/blog/2020/06/09/apache-kafka-versus-apache-pulsar-event-streaming-comparison-features-myths-explored/#myth-6-pulsars-performance-is-much-better-than-kafkas target=_blank>Kafka</a> when accessing messages outside of a broker’s managed ledger. It also can’t guarantee exactly-once processing, and its native solution for stream processing is limited to simple functionality.</p><p>Pulsar is a strong choice for microservices, autonomous vehicles, instant messaging, and analytics.</p></article><div class="row border border-primary rounded"><div class="col p-2"><h3>You might also like...</h3><div class=row><div class="col text-center p-2"><div><a href=/articles/comparing-apache-pulsar-and-rabbitmq/><img src=https://user-images.githubusercontent.com/16946028/176910739-60d5f639-6c52-4649-992a-fb1c9d1aadb9.png alt class=img-fluid style=max-width:150px></a></div><div class=pt-2><a href=/articles/comparing-apache-pulsar-and-rabbitmq/>Comparing Apache Pulsar and RabbitMQ</a></div><div class=pt-2>Let’s take an in-depth look at the similarities and differences between Pulsar and RabbitMQ</div></div><div class="col text-center p-2"><div><a href=/articles/rest-versus-event-driven-architecture-why-it-s-time-to-switch-from-request-based-architecture/><img src=https://user-images.githubusercontent.com/16946028/175393332-fbd80156-a0a1-4f96-84a2-56d01e6bc16e.png alt class=img-fluid style=max-width:150px></a></div><div class=pt-2><a href=/articles/rest-versus-event-driven-architecture-why-it-s-time-to-switch-from-request-based-architecture/>REST Versus Event Driven Architecture - Why It's Time to Switch From Request Based Architecture</a></div><div class=pt-2>This article introduces REST and event-driven architectures and explains why organizations should …</div></div><div class="col text-center p-2"><div><a href=/articles/why-managed-apache-pulsar-is-the-right-choice/><img src=https://user-images.githubusercontent.com/16946028/175392736-555608d6-d5cb-426a-b41a-7b2babadff9c.png alt class=img-fluid style=max-width:150px></a></div><div class=pt-2><a href=/articles/why-managed-apache-pulsar-is-the-right-choice/>Why Managed Apache Pulsar Is the Right Choice</a></div><div class=pt-2>In this article, we’ll explore why and how using a managed Apache Pulsar service saves you time and …</div></div></div></div></div><br><div id=disqus_thread></div><script type=text/javascript>(function(){if(window.location.hostname=="localhost")return;var t,e=document.createElement("script");e.type="text/javascript",e.async=!0,t="pulsar-neighborhood",e.src="//"+t+".disqus.com/embed.js",(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(e)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript></div></div><div class="col-0 col-md-2"><nav class="navbar navbar-light mb-3"><div class=row><div class=col-12><h4>In this article</h4></div><div class="col-12 border-left border-primary pr-0"><nav id=TableOfContents><ul><li><a href=#messaging>Messaging</a></li><li><a href=#kafka>Kafka</a><ul><li><a href=#kafka-architecture>Kafka Architecture</a></li><li><a href=#implications-for-streaming>Implications for Streaming</a></li><li><a href=#kafka-stream-processing>Kafka Stream Processing</a></li></ul></li><li><a href=#pulsar>Pulsar</a><ul><li><a href=#pulsar-architecture>Pulsar Architecture</a></li><li><a href=#persistent-messaging>Persistent Messaging</a></li><li><a href=#implications-for-streaming-1>Implications for Streaming</a></li><li><a href=#pulsar-stream-processing>Pulsar Stream Processing</a></li></ul></li><li><a href=#conclusion>Conclusion</a></li></ul></nav></div></div></nav><div class="row border border-primary rounded no-gutters"><div class="col-12 bg-primary p-1"><div class=text-secondary>&nbsp;Join Our Newsletter</div></div><div class=col-12><div class=ctct-inline-form data-form-id=7514f5a0-b12d-44b3-93bf-e16449e783ca style=padding:0></div></div></div></div></div></div></div><div class="sub-footer m-5"><div class=row><div class="col-12 text-center"><a href=https://github.com/pulsar-neighborhood/pulsar-neighborhood.github.io/blob/main/content/articles/Comparing-Apache-Pulsar-Streaming-and-Kafka-Streaming.md target=_blank>Improve this page</a></div></div><div class="col-8 offset-2"><hr class="bg-primary text-primary"></div><div class="sub-footer-inner mt-2"><div><div class=mb-1>Pulsar Neighborhood</div><div><ul><li style=width:40px><a href=https://github.com/pulsar-neighborhood target=_blank><img src=/svg/github-brands.svg title="Pulsar Neighborhood on GitHub" class=img-fluid></a></li><li class="ml-3 mr-1" style=width:40px><a href=https://twitter.com/pulsar_neighbor target=_blank><img src=/svg/twitter-brands.svg title="Pulsar Neighborhood on Twitter" class=img-fluid></a></li><li style=width:40px><a href=https://www.youtube.com/c/ApachePulsarNeighborhood target=_blank><img src=/svg/youtube-brands.svg title="Pulsar Neighborhood on YouTube" class=img-fluid></a></li><li style=width:40px><a href=https://datastudio.google.com/reporting/743d9f31-c80f-4e60-b520-730631d3c250 target=_blank><img src=/png/google-analytics.png title="Pulsar Neighborhood analytics" class=img-fluid></a></li></ul></div></div><div class=p-5></div><div><div class=mb-1>Apache Pulsar</div><div><ul><li style=width:40px><a href=https://pulsar.apache.org target=_blank><img src=/png/pulsar-icon.jpg title="Apache Pulsar project home" class=img-fluid></a></li><li class="ml-3 mr-1" style=width:40px><a href=https://github.com/apache-pulsar target=_blank><img src=/svg/github-brands.svg title="Apache Pulsar on GitHub" class=img-fluid></a></li><li class=mr-1 style=width:40px><a href=https://twitter.com/Apache_Pulsar target=_blank><img src=/svg/twitter-brands.svg title="Apache Pulsar on Twitter" class=img-fluid></a></li><li class=mr-1 style=width:30px><a href=https://stackoverflow.com/questions/tagged/apache-pulsar target=_blank><img src=/svg/stack-overflow-brands.svg title="Apache Pulsar on Stackoverflow" class=img-fluid></a></li><li style=width:40px><a href=https://apache-pulsar.slack.com target=_blank><img src=/svg/slack-brands.svg title="Apache Pulsar slack channel" class=img-fluid></a></li></ul></div></div></div><div class="row text-center mb-3"><div class=col-12><a class="btn btn-info" href=https://pulsar.apache.org/community/ target=_blank>Join the Pulsar community</a></div></div><div class="row text-center"><div class="col-12 text-center"><div><a href=https://github.com/pulsar-neighborhood/pulsar-neighborhood.github.io/issues/new/choose>Contact Us</a></div></div></div></div><script type=text/javascript src=/js/scripts.min.8efe3e711319c2d0a300a084134e3f3bedcb60dce21f1e0ef12767f654242b44.js></script>
<script type=text/javascript src=/js/filter.min.2ee09abe7fb6b636d1e7cc4506185cec9d630337089eff4daf0ccc27318f376e.js></script>
<script src=https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js></script>
<script src=https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js integrity=sha384-fQybjgWLrvvRgtW6bFlB7jaZrFsaBXjsOMm/tB9LTS58ONXgqbR9W8oWht/amnpF crossorigin=anonymous></script>
<script>var _ctct_m="310a7590faa964361197c4644ef1de96"</script><script id=signupScript src=//static.ctctcdn.com/js/signup-form-widget/current/signup-form-widget.min.js async defer></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-8S9SD43SBH"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-8S9SD43SBH")</script></body></html>